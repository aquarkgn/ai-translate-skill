# Qwen2.5-14B 翻译能力边界深度测试报告

## 1. 测试背景与目标
针对本地部署的 `Qwen2.5-14B-Instruct-8bit` (MLX/Ollama) 进行量化测试，探测其在处理 i18n JSON 翻译任务时，不同批处理量（Batch Size）下的稳定性、完整性及其失效边界的根本原因。

## 2. 实验数据汇总

| 批次大小 (BS) | 状态 | Key 完整度 | 平均响应耗时 | 结果分析 |
| :--- | :--- | :--- | :--- | :--- |
| **20** | ✅ **通过** | 100% | ~25s | 结构严整，翻译自然，无任何项丢失。 |
| **30** | ✅ **通过** | 100% | ~35s | 表现稳定，依然能保持全量输出。 |
| **50** | ❌ **失败** | ~38% | ~50s | 出现大规模 "丢失项"，模型仅返回了前 19 条数据即产生截断或跳跃。 |
| **80** | ❌ **严重失败** | ~31% | ~60s | 丢失比例进一步扩大，返回内容呈现“开头+结尾”或“随机采样”特征。 |
| **100** | ❌ **崩溃/超时** | <10% | - | 极易触发输出 Token 限制或解析异常。 |

## 3. 逐轮边界能力与成因分析

### Round 1: BS 20 (安全区)
- **表现**: 完美遵循指令，Key 完全匹配。
- **成因**: 20 个 JSON 字段及其对应 Value 的总长度约为 1000-1500 Tokens（含 Prompt）。这在 14B 模型的密集注意力范围内，模型能清晰定位每一个 Key 并在单次生成窗口内完成任务。

### Round 2: BS 50 (结构损耗临界点)
- **现象**: 仅返回了请求项中约 1/3 的内容，且没有报错，模型似乎“认为”自己已经写完了。
- **技术成因**:
    1. **注意力摊薄 (Attention Dilution)**: 随着输入字段增多，Self-Attention 机制在处理生成任务时，对距离较远的 Prompt 指令引用强度下降。
    2. **Token 预算压力**: 50 个复杂字段的翻译输出往往逼近 2000+ Tokens。虽然 `max_tokens` 为 4096，但在本地 8-bit 量化环境下，长文本生成的质量往往在 1500-2000 Tokens 后开始劣化。
    3. **策略性偷懒**: 模型为了逻辑自洽，在发现“活干不完”时，倾向于省略中间步骤直接输出结尾的 JSON 括号，导致中间字段丢失。

### Round 3: BS 80-100 (指令崩溃区)
- **现象**: JSON 语法可能依然正确，但内容与原文 Key 的映射关系几乎脱节。
- **技术成因**:
    1. **上下文窗口管理失效**: 尽管模型标称 32k/128k 窗口，但在 8-bit 量化下，KV Cache 的精度损失使得模型在处理同构（多个类似的 JSON Key）任务时产生严重的“语义混淆”。
    2. **概率采样漂移**: 在超长文本生成过程中，Token 预测的累积错误导致模型偏离了原有的 JSON 模板轨迹。

## 4. 核心结论
`Qwen2.5-14B` 模型在翻译任务中的 **物理边界** 约在 30-40 项之间。
- **建议阈值**: 为了 100% 的数据安全性，最高不应超过 **20-25**。
- **便捷性优势**: 在该阈值内，14B 模型展现了远超 7B 及以下模型的语感和复杂占位符（如 `{{count}}`）保护能力。

## 5. 优化建议
1. **多级 Batch 策略**: 对于关键、长文本，强制 BS=10；对于短按钮、短语，可放宽至 BS=30。
2. **逻辑分块**: 在 Prompt 中增加索引号（如 `idx_0`）有助于模型在长生成过程中保持位置感。
3. **输出二次校验**: 在 Agent 技能中内置 Key 缺失自动补重译的逻辑。
